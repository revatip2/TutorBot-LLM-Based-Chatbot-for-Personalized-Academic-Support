{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT"
      ],
      "metadata": {
        "id": "akGu66QSPDWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the tokenizer and pre-trained model.\n",
        "\n",
        "2. Load and preprocess the dataset.\n",
        "\n",
        "3. Randomly split the dataset into training and evaluation sets.\n",
        "\n",
        "4. Define training arguments, train the model, and save it.\n",
        "\n",
        "5. Classify questions using the trained model.\n",
        "\n",
        "6. Load the model and classify more questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "tmfoFDAiSUhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the pre-trained BERT model with 2 labels (logistics and course-specific)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Final Project/backup - ques_cat.csv')\n",
        "\n",
        "# Label encoding (converting string labels to integers)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Check label mapping\n",
        "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
        "print(label_mapping)\n",
        "\n",
        "# Define the preprocessing function to tokenize the text\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"question\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert the DataFrame into a Hugging Face Dataset object\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Randomly split the dataset into train and evaluation sets\n",
        "train_test_split = encoded_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer object for training and evaluation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model's state dictionary\n",
        "model_save_path = '/content/drive/MyDrive/Final Project/bert_model.bin'\n",
        "torch.save(trainer.model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "yLOsrSlBTL4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5pnnl7jPBpY"
      },
      "outputs": [],
      "source": [
        "def bert_mod(user_question):\n",
        "    # Load BERT Tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    # Load BERT Model\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "    model.load_state_dict(torch.load(bert_model_path , map_location='cpu'))\n",
        "    model.eval()\n",
        "    category = classify_question(user_question, model, tokenizer)\n",
        "    return category\n",
        "\n",
        "# Function to classify new questions\n",
        "def classify_question(question, model, tokenizer, device='cpu'):\n",
        "    model.to(device)\n",
        "    # Tokenize the question\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get probabilities using softmax and find the category with the highest probability\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu()\n",
        "    category = np.argmax(predictions.numpy())\n",
        "\n",
        "    return category"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Video Transcription: OpenAI Whisper"
      ],
      "metadata": {
        "id": "4i2QslT2ns6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import json\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import *\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HT91GRJPnvel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create Dataset of Video URLs."
      ],
      "metadata": {
        "id": "x0WjgS8tn2zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the following with your API key and playlist ID\n",
        "api_key = 'AIzaSyBHGcRxLtGi0Gb6QYlYVB7aRujxG2A3uxg'\n",
        "playlist_id = 'PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "def convert_seconds_to_timestamp(seconds):\n",
        "    seconds = float(seconds)\n",
        "    hh = int(seconds // 3600)\n",
        "    mm = int((seconds % 3600) // 60)\n",
        "    ss = int(seconds % 60)\n",
        "    timestamp = f\"{hh:02}:{mm:02}:{ss:02}.000\"\n",
        "\n",
        "    return timestamp\n",
        "\n",
        "def get_playlist_video_details(playlist_id):\n",
        "    video_details = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        pl_request = youtube.playlistItems().list(\n",
        "            part='contentDetails',\n",
        "            playlistId=playlist_id,\n",
        "            # maxResults=50,  # Adjust as needed, max is 50\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "\n",
        "        pl_response = pl_request.execute()\n",
        "\n",
        "        video_ids = [item['contentDetails']['videoId'] for item in pl_response['items']]\n",
        "\n",
        "        # If you have a large number of videos, consider splitting this into multiple requests to avoid going over quota\n",
        "        videos_request = youtube.videos().list(\n",
        "            part=\"snippet\",\n",
        "            id=','.join(video_ids)\n",
        "        )\n",
        "\n",
        "        videos_response = videos_request.execute()\n",
        "\n",
        "        for item in videos_response['items']:\n",
        "            video_details.append({\n",
        "                'title': item['snippet']['title'],\n",
        "                'url': f\"https://www.youtube.com/watch?v={item['id']}\"\n",
        "            })\n",
        "\n",
        "        next_page_token = pl_response.get('nextPageToken')\n",
        "\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return video_details\n",
        "\n",
        "# Fetch video details\n",
        "video_details = get_playlist_video_details(playlist_id)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(video_details)\n",
        "csv_filename = 'lecture_video_urls.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\"Saved playlist data to {csv_filename}\")\n"
      ],
      "metadata": {
        "id": "EH8h0UgGn1pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download each video, get transcript using Whisper model, and save the timestamps. Iterate over each timestamp, and capture the sreenshots from the video. SO when the timestamps are returned, we also fetch the screenshot for visual reference."
      ],
      "metadata": {
        "id": "oaXlIjeloFoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download YouTube video\n",
        "def download_video(url, path='videos/'):\n",
        "    yt = YouTube(url)\n",
        "    ys = yt.streams.get_highest_resolution()\n",
        "    print(\"Downloading...\", yt.title)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    filename = ys.download(output_path=path)\n",
        "    print(\"Download completed\\n\")\n",
        "    return yt.title, filename\n",
        "\n",
        "# Get video transcript with timestamps\n",
        "def get_transcript(video_path, model, output_path='transcripts/'):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    print(\"Transcribing video..\")\n",
        "\n",
        "    transcription = model.transcribe(video_path)\n",
        "\n",
        "    data = dict()\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        data[segment['id']] = {'start':segment['start'], 'end':segment['end'], 'text':segment['text']}\n",
        "    with open(output_path+video_path[video_path.rindex(\"/\")+1:-4]+'_transcript.json', 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(\"Transcription saved\\n\")\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Take screenshots at given timestamps\n",
        "def capture_screenshots(video_path, timestamps, output_path='screenshots/'):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    print(\"Taking screenshots..\")\n",
        "    clip = VideoFileClip(video_path)\n",
        "    for timestamp in timestamps:\n",
        "        imgpath = os.path.join(output_path, f\"{video_path[video_path.rindex(\"/\")+1:-4]}_screenshot_{timestamp}.png\")\n",
        "        clip.save_frame(imgpath, t=timestamp)\n",
        "    print(\"Screenshots saved\\n\")\n",
        "\n",
        "\n",
        "videos = pd.read_csv('lecture_video_urls.csv')['url']\n",
        "transcription_dict = dict()\n",
        "\n",
        "for video_url in videos:\n",
        "    title, video_path = download_video(video_url)\n",
        "    model = whisper.load_model(\"base.en\")\n",
        "    transcription = get_transcript(video_path, model)\n",
        "    transcription_dict[title] = transcription['text']\n",
        "    timestamps = [seg['start'] for seg in transcription['segments']]\n",
        "    try:\n",
        "        capture_screenshots(video_path, timestamps)\n",
        "    except:\n",
        "        print(\"Error occurred\")\n",
        "\n",
        "transcription_df = pd.DataFrame(list(transcription_dict.items()), columns=['Topic', 'Answer'])\n",
        "transcription_df.to_csv('transcriptions.csv', index=False)"
      ],
      "metadata": {
        "id": "7ST5kts8oDAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Video Screenshots every 1 Second."
      ],
      "metadata": {
        "id": "V38P_T4VrBWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video(url, path='videos/'):\n",
        "    yt = YouTube(url)\n",
        "    ys = yt.streams.get_highest_resolution()\n",
        "    print(\"Downloading...\", yt.title)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    filename = ys.download(output_path=path)\n",
        "    print(\"Download completed\\n\")\n",
        "    return yt.title, filename\n",
        "\n",
        "def capture_screenshots(video_path, output_path='screenshots_one_second/'):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    print(\"Taking screenshots..\")\n",
        "\n",
        "    clip = VideoFileClip(video_path)\n",
        "    duration = clip.duration\n",
        "    for timestamp in range(int(duration)):\n",
        "        imgpath = os.path.join(output_path, f\"{os.path.basename(video_path)[:-4]}_screenshot_{timestamp}.png\")\n",
        "        clip.save_frame(imgpath, t=timestamp)\n",
        "\n",
        "    clip.close()\n",
        "\n",
        "    print(\"Screenshots saved\\n\")\n",
        "\n",
        "\n",
        "videos = pd.read_csv('lecture_video_urls.csv')['url'][89:]\n",
        "for video_url in videos:\n",
        "    title, video_path = download_video(video_url)\n",
        "    try:\n",
        "        capture_screenshots(video_path)\n",
        "    except:\n",
        "        print(\"Error occurred\")"
      ],
      "metadata": {
        "id": "qe7pKItVrLkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Compare each screenshot."
      ],
      "metadata": {
        "id": "mDaCfMUBCp09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ss_dir = \"screenshots_timestamps\"\n",
        "ss_files = os.listdir(ss_dir)\n",
        "l1_ss = [os.path.join(ss_dir,file) for file in ss_files if \"Lecture 1 — Distributed File Systems\" in file]\n",
        "print(\"Number of screenshots captured at transcript timestamps:\",len(l1_ss))\n",
        "print(\"Eg:\",l1_ss[0])\n",
        "\n",
        "l1_ss_dict = dict()\n",
        "for ss in l1_ss:\n",
        "    l1_ss_dict[float(ss[ss.find('screenshot_')+11:-4])] = ss\n",
        "l1_ss_dict = dict(sorted(l1_ss_dict.items()))\n",
        "\n",
        "ss1_dir = \"screenshots_one_second\"\n",
        "ss1_files = os.listdir(ss1_dir)\n",
        "l1_ss1 = [os.path.join(ss1_dir,file) for file in ss1_files if \"Lecture 1 — Distributed File Systems\" in file]\n",
        "print(\"Number of screenshots captured at every second:\",len(l1_ss1))\n",
        "print(\"Eg:\",l1_ss1[0])\n",
        "\n",
        "l1_ss1_dict = dict()\n",
        "for ss in l1_ss1:\n",
        "    l1_ss1_dict[float(ss[ss.find('screenshot_')+11:-4])] = ss\n",
        "l1_ss1_dict = dict(sorted(l1_ss1_dict.items()))\n",
        "\n",
        "for time, ss in l1_ss_dict.items():\n",
        "    img_timestamp = Image.open(ss)\n",
        "    print(ss)\n",
        "    closest_second = int(time)\n",
        "    img_one_second = Image.open(l1_ss1_dict[closest_second])\n",
        "    print(l1_ss1_dict[closest_second])\n",
        "\n",
        "    pixels_timestamp = list(img_timestamp.getdata())\n",
        "    pixels_one_second = list(img_one_second.getdata())\n",
        "\n",
        "    print(pixels_timestamp == pixels_one_second)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "6aZf-z_4Cvv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Compare pixels and remove redundant images."
      ],
      "metadata": {
        "id": "Xz3pAZpwDCbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retained_ss = dict()\n",
        "times = l1_ss1_dict.keys()\n",
        "time1 = 0\n",
        "while time1 < len(times)-1:\n",
        "    i = time1\n",
        "    ss1 = l1_ss1_dict[time1]\n",
        "    img1 = Image.open(ss1)\n",
        "    pixels1 = list(img1.getdata())\n",
        "\n",
        "    retained_ss[time1] = ss1\n",
        "\n",
        "    for j in range(i+1, len(times)):\n",
        "        time2 = j\n",
        "        ss2 = l1_ss1_dict[time2]\n",
        "        img2 = Image.open(ss2)\n",
        "        pixels2 = list(img2.getdata())\n",
        "        print(\"i\",i,\"j\",j)\n",
        "\n",
        "        differences = [tuple(abs(c1 - c2) for c1, c2 in zip(pixel1, pixel2))\n",
        "                   for pixel1, pixel2 in zip(pixels1, pixels2)]\n",
        "\n",
        "        avg_difference = tuple(sum(c) / len(differences) for c in zip(*differences))\n",
        "        if any(value > 50 for value in avg_difference):\n",
        "            print(\"different\")\n",
        "            time1 = time2\n",
        "            break\n",
        "        elif time2==len(times)-1:\n",
        "            time1 = time1+1\n",
        "            break\n",
        "        else:\n",
        "            print(\"same\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "BM4Lp2QUDEv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}